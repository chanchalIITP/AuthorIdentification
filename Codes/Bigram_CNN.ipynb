{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bigram_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRSjjm3bOD5N"
      },
      "source": [
        "import os\n",
        "import math\n",
        "from termcolor import colored\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import pandas as pd\n",
        "#from data_helpers import load_data\n",
        "from keras import callbacks\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.layers import Embedding\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Dropout, Flatten\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "\n",
        "import regex\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, AveragePooling1D\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJMj3CD3IzKb"
      },
      "source": [
        "import pandas as pd\n",
        " \n",
        "dataset=pd.read_csv('/content/50_tweets_per_user.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NoJMKjwJC9H"
      },
      "source": [
        "k=0\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "for i in range(0,50):\n",
        "  for j in range(k,k+45):\n",
        "    X_train.append(dataset.iloc[j,0])\n",
        "    y_train.append(dataset.iloc[j,1])\n",
        "  k+=50"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2u8BqY8JJx_"
      },
      "source": [
        "k=45\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "for i in range(0,50):\n",
        "  for j in range(k,k+5):\n",
        "    X_test.append(dataset.iloc[j,0])\n",
        "    y_test.append(dataset.iloc[j,1])\n",
        "  k+=50"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHd59nwWJJ3g"
      },
      "source": [
        "X_train=pd.DataFrame(X_train)\n",
        "X_test=pd.DataFrame(X_test)\n",
        "y_train=pd.DataFrame(y_train)\n",
        "y_test=pd.DataFrame(y_test)\n",
        "X_train=X_train.iloc[:,:].values\n",
        "X_test=X_test.iloc[:,:].values\n",
        "y_train=y_train.iloc[:,:].values\n",
        "y_test=y_test.iloc[:,:].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIZLMLDIJJ-V"
      },
      "source": [
        "train=np.concatenate((X_train,y_train),axis=1)\n",
        "test=np.concatenate((X_test,y_test),axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqfcvH9MJKKj"
      },
      "source": [
        "np.random.shuffle(train)\n",
        "np.random.shuffle(test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPd3i-KJKIM"
      },
      "source": [
        "train=pd.DataFrame(train)\n",
        "test=pd.DataFrame(test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a0Blff_JKFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be649cb0-59a7-4c01-8279-f1dddc60cabc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaRJaNTJJKCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b49956-5415-41a4-c6eb-fbcb7849231f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMuj-Q8NJJ8r"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "def preprocess(sentence):\n",
        "  sentence=str(sentence)\n",
        "  sentence = sentence.lower()\n",
        "#  sentence=sentence.replace('{html}',\"\") \n",
        "#  cleanr = re.compile('<.*?>#@')\n",
        "#  cleantext = re.sub(cleanr, '', sentence)\n",
        "#  rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "#  rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "#  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "#  tokens = tokenizer.tokenize(rem_num)  \n",
        "#  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "#  stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "#  lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "  return sentence\n",
        "\n",
        "\n",
        "train.iloc[:,0]=train.iloc[:,0].map(lambda s:preprocess(s))\n",
        "test.iloc[:,0]=test.iloc[:,0].map(lambda s:preprocess(s))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoUoIcktJJ6t"
      },
      "source": [
        "X_train=train.iloc[:,0]\n",
        "X_test=test.iloc[:,0]\n",
        "y_train=train.iloc[:,1]\n",
        "y_test=test.iloc[:,1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGgdpm29JJ1B"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_Y = encoder.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_train = np_utils.to_categorical(encoded_Y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCm-YVdEJw8P"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_test)\n",
        "encoded_Y = encoder.transform(y_test)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_test = np_utils.to_categorical(encoded_Y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdJ7eP_OG1D"
      },
      "source": [
        "def create_vocab_set2():\n",
        "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
        "                list(string.punctuation) + ['\\n'] + [' '] )\n",
        "    \n",
        "    print('alphabet', alphabet)\n",
        "    #x = input(print('enter'))\n",
        "    alphabets = []\n",
        "    for i in range(0, len(alphabet)):\n",
        "        for j in range(0, len(alphabet)):\n",
        "              alphabets.append(alphabet[i]+alphabet[j])\n",
        "    #print(alphabets)\n",
        "    #x = input(print('enter'))\n",
        "\n",
        "\n",
        "    vocab_size = len(alphabets)\n",
        "    check = set(alphabets)\n",
        "\n",
        "    vocab = {}\n",
        "    reverse_vocab = {}\n",
        "    for ix, t in enumerate(alphabets):\n",
        "        vocab[t] = ix\n",
        "        reverse_vocab[ix] = t\n",
        "\n",
        "    return vocab, reverse_vocab, vocab_size, check\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYkbCoPaJxJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50267ecd-e8a1-4bae-881a-c4dbfb454dd1"
      },
      "source": [
        "maxlen = 140\n",
        "vocab, reverse_vocab, vocab_size, check =create_vocab_set2()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alphabet ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\n', ' ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTD_7n9JJw_9"
      },
      "source": [
        "def encode_data2(x, maxlen, vocab, vocab_size, check):              # For character embedding use this function instead of encode_data\n",
        "\n",
        "    input_data = np.zeros((len(x), maxlen))\n",
        "    for dix, sent in enumerate(x):\n",
        "        counter = 0\n",
        "        chars = list(sent.lower().replace(\" \", \"\"))\n",
        "        chars2 = []\n",
        "        for i in range(len(chars)-1):\n",
        "            chars2.append(chars[i] + chars[i+1])\n",
        "        for i,c in enumerate(chars2):\n",
        "            if counter >= maxlen:\n",
        "                pass\n",
        "            else:\n",
        "                if c in check:\n",
        "                    counter += 1\n",
        "                    ix = vocab[c]\n",
        "                    input_data[dix,counter-1] = ix\n",
        "\n",
        "    return input_data "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttslg2HhJxDk"
      },
      "source": [
        "X_train = encode_data2(X_train, maxlen, vocab, vocab_size, check)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuAWRhJQMEq4"
      },
      "source": [
        "X_test= encode_data2(X_test, maxlen, vocab, vocab_size, check)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Z2aGKdPx88"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X_train,y_train, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN8Ysq7wP8wP"
      },
      "source": [
        "def model2(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter,cat_output):                                                  # For Character Embedding use this model instead of above model\n",
        "    d = 300                                                             #Embedding Size\n",
        "    Embedding_layer  = Embedding(vocab_size+1, d, input_length=maxlen)\n",
        "    inputs = Input(shape=(maxlen,), name='input', dtype='float32')\n",
        "    embed = Embedding_layer(inputs)\n",
        "    z = Dropout(0.25)(Dense(dense_outputs, activation='relu')(embed))\n",
        "    conv = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0], activation='relu',input_shape=(maxlen, d))(z)\n",
        "    conv = MaxPooling1D(pool_size=3)(conv)\n",
        "\n",
        "    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1], activation='relu')(conv)\n",
        "    conv1 = MaxPooling1D(pool_size=3)(conv1)\n",
        "\n",
        "    conv2 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2], activation='relu')(conv1)\n",
        "    conv5 = MaxPooling1D(pool_size=3)(conv2)\n",
        "    conv = Flatten()(conv5)\n",
        "\n",
        "    #Two dense layers with dropout of .5\n",
        "    #z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv))\n",
        "    # z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
        "\n",
        "    pred = Dense(cat_output, activation='softmax', name='output')(conv)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=pred)\n",
        "\n",
        "    #sgd = SGD(lr=0.001, momentum=0.9)\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP_JgdCP_0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8670c5-9e7e-48de-8f10-cb71c67b634b"
      },
      "source": [
        "    nb_filter = 500\n",
        "    dense_outputs = 256\n",
        "    filter_kernels = [3,4,5]\n",
        "    cat_output = 2\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "    model = model2(filter_kernels, dense_outputs,maxlen, vocab_size, nb_filter, cat_output)\n",
        "    model.summary()\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 140)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 140, 300)          1470300   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 140, 256)          77056     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 140, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 138, 500)          384500    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 46, 500)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 43, 500)           1000500   \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 14, 500)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 10, 500)           1250500   \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 3, 500)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1500)              0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 2)                 3002      \n",
            "=================================================================\n",
            "Total params: 4,185,858\n",
            "Trainable params: 4,185,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEMr38ngQhng"
      },
      "source": [
        "adam1=Adam(lr=1.00, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eozvovQIZBn4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "6a6109bb-ea32-4a53-8e27-781a2c84ec19"
      },
      "source": [
        "model.fit(np.array(X_train), np.array(y_train),validation_data=[np.array(X_eval), np.array(y_eval)],batch_size=32,epochs=15, verbose=1, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1470 samples, validate on 490 samples\n",
            "Epoch 1/15\n",
            "1470/1470 [==============================] - 31s 21ms/step - loss: 0.3405 - accuracy: 0.8701 - val_loss: 0.1516 - val_accuracy: 0.9510\n",
            "Epoch 2/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 0.1107 - accuracy: 0.9660 - val_loss: 0.1404 - val_accuracy: 0.9612\n",
            "Epoch 3/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 0.0313 - accuracy: 0.9898 - val_loss: 0.1475 - val_accuracy: 0.9571\n",
            "Epoch 4/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.2181 - val_accuracy: 0.9510\n",
            "Epoch 5/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 6.5361e-04 - accuracy: 1.0000 - val_loss: 0.2149 - val_accuracy: 0.9592\n",
            "Epoch 6/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 6.4967e-04 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9531\n",
            "Epoch 7/15\n",
            "1470/1470 [==============================] - 30s 21ms/step - loss: 8.5103e-05 - accuracy: 1.0000 - val_loss: 0.2511 - val_accuracy: 0.9592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6eca665940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDTJl3dvQPol"
      },
      "source": [
        "  y_pred = model.predict(np.array(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTjzccxfTnH4"
      },
      "source": [
        "y_pred=pd.DataFrame(y_pred)\n",
        "y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)\n",
        "y_pred=y_pred.iloc[:,:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l3t1MknTqgj"
      },
      "source": [
        "y_test=pd.DataFrame(y_test)\n",
        "y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)\n",
        "y_test=y_test.iloc[:,:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbM8tI-eWZwA"
      },
      "source": [
        "result=[]\n",
        "for i in range(0,len(y_test)):\n",
        "  for j in range(0,len(y_test[0])):\n",
        "    if(y_test[i][j]==1):\n",
        "      result.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uXF999cWcRK"
      },
      "source": [
        "predicted=[]\n",
        "for i in range(0,len(y_pred)):\n",
        "  for j in range(0,len(y_pred[0])):\n",
        "    if(y_pred[i][j]==1):\n",
        "      predicted.append(j)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_vgxgIrWfhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e72e89c9-e0c5-4bb7-c382-c57e515c5750"
      },
      "source": [
        "print(result)\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4w75XoFWhKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "25c781ca-1232-43cf-a6c6-dab405207228"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(result,predicted)\n",
        "\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[20,  0],\n",
              "       [ 0, 20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUQ3-ZKqWjEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "a4167c00-f332-4e10-fd11-f8c5bcdb0230"
      },
      "source": [
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "print('Confusion Matrix :')\n",
        "print(cm) \n",
        "print('Accuracy Score :',accuracy_score(result, predicted)) \n",
        "print('Report : ')\n",
        "print(classification_report(result, predicted)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "Accuracy Score : 1.0\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        20\n",
            "           1       1.00      1.00      1.00        20\n",
            "\n",
            "    accuracy                           1.00        40\n",
            "   macro avg       1.00      1.00      1.00        40\n",
            "weighted avg       1.00      1.00      1.00        40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdCh6VZCxNHR"
      },
      "source": [
        "k=980\n",
        "X_train1=[]\n",
        "y_train1=[]\n",
        "for i in range(0,2):\n",
        "  for j in range(k,k+20):\n",
        "    X_train1.append(dataset.iloc[j,0])\n",
        "    y_train1.append(dataset.iloc[j,1])\n",
        "  k+=1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBtOo5jhHdwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cd4a99b-5476-46d9-f5bb-6f3751c1c2c9"
      },
      "source": [
        "from keras import backend as K1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emlhTdlnvFUj"
      },
      "source": [
        "file=open(\"heatmap_authorshipatribution2_new.html\",\"w\")\n",
        "for i in range(0,40):\n",
        "  type_here=[]\n",
        "  type_here.append(X_train1[i])\n",
        "  typr_here=pd.DataFrame(type_here)\n",
        "  typr_here = encode_data2(type_here, maxlen, vocab, vocab_size, check)\n",
        "  y_pred = model.predict(typr_here)\n",
        "  Xtst=typr_here\n",
        "  class_idx = np.argmax(y_pred[0]) #not needed in this case as only two classes\n",
        "  class_output = model.output[:, class_idx]\n",
        "  last_conv_layer = model.get_layer(\"conv1d_1\")\n",
        "  grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "  pooled_grads = K.mean(grads)\n",
        "  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "  pooled_grads_value, conv_layer_output_value = iterate([Xtst])\n",
        "  \n",
        "\n",
        "  heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "  heatmap = np.maximum(heatmap,0)\n",
        "  heatmap /= np.max(heatmap)#normalise values in the prediction\n",
        "  norm_len = 36/last_conv_layer.output_shape[1]\n",
        "  html = \"\"\n",
        "  if y_pred[0][0]>0.5:\n",
        "    pred = '90078731'\n",
        "  else:\n",
        "    pred = '51964081'\n",
        "  html += \"<span><h3>Based on the description, the model believes that text belongs to {} author \".format(pred)\n",
        "  html += \"<small><br>Confidence: {:.0f}%<br><br></small></h3></span>\".format(abs(((y_pred[0][0]*100)-50)*2))\n",
        "  for j,i in enumerate(type_here[0].split()):\n",
        "    html += \"<span style='background-color:rgba({},0,15,{})'>{} </span>\".format(heatmap[math.floor(j/norm_len)]*255,heatmap[math.floor(j/norm_len)]-0.3,i)\n",
        "  file.write(html)\n",
        "file.close()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtJ3wExuA8cP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "ee33c8a8-106a-4bae-aa95-8efc920557f2"
      },
      "source": [
        " html = \"\"\n",
        "if y_pred[0]>0.5:\n",
        "  pred = '90078731'\n",
        "else:\n",
        "  pred = '15401533'\n",
        "html += \"<span><h3>Based on the description, the model believes that text belongs to {} author \".format(pred)\n",
        "html += \"<small><br>Confidence: {:.0f}%<br><br></small></h3></span>\".format(abs(((y_pred[0][0]*100)-50)*2))\n",
        "for j,i in enumerate(type_here[0].split()):\n",
        "  html += \"<span style='background-color:rgba({},0,150,{})'>{} </span>\".format(heatmap[math.floor(j/norm_len)]*255,heatmap[math.floor(j/norm_len)]-0.3,i)\n",
        "\n",
        "HTML(html)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<span><h3>Based on the description, the model believes that text belongs to 90078731 author <small><br>Confidence: 100%<br><br></small></h3></span><span style='background-color:rgba(87.8972178697586,0,150,0.044694972038269054)'>Now </span><span style='background-color:rgba(124.70195174217224,0,150,0.1890272617340088)'>playing: </span><span style='background-color:rgba(87.75895088911057,0,150,0.044152748584747326)'>Hermes </span><span style='background-color:rgba(113.95427405834198,0,150,0.14687950611114503)'>House </span><span style='background-color:rgba(106.24319583177567,0,150,0.11663998365402223)'>Band </span><span style='background-color:rgba(105.93030542135239,0,150,0.11541296243667604)'>- </span><span style='background-color:rgba(132.08886176347733,0,150,0.21799553632736207)'>Country </span><span style='background-color:rgba(105.31387969851494,0,150,0.1129956066608429)'>roads. </span><span style='background-color:rgba(132.90410041809082,0,150,0.2211925506591797)'>Tune </span><span style='background-color:rgba(164.05060350894928,0,150,0.3433357000350952)'>in: </span><span style='background-color:rgba(95.62608674168587,0,150,0.07500426173210145)'>http://stream.laut.fm/eurodance.m3u </span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}