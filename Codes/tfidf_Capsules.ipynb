{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_Capsules.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjWthQ81JT0A"
      },
      "source": [
        "#Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.keras import activations, regularizers, initializers, constraints, engine\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.keras.layers import Layer, deserialize, Conv1D\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.keras.models import Sequential,Model\n",
        "from tensorflow.python.keras.layers import Flatten, Dense,AveragePooling1D,GRU,Convolution1D, MaxPooling1D, AveragePooling1D,Embedding,Input, Dense, Dropout, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmeQkEr-N2zh"
      },
      "source": [
        "\n",
        "# The Capsule Architecture\n",
        "gru_len = 128\n",
        "Routings = 5\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "dropout_p = 0.3\n",
        "rate_drop_dense = 0.3\n",
        "\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "# define our own softmax function instead of K.softmax\n",
        "# because K.softmax can not specify axis.\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "# define the margin loss like hinge loss\n",
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1\n",
        "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
        "\n",
        "\n",
        "class Capsule(Layer):\n",
        "\n",
        "    \"\"\"A Capsule Implement with Pure Keras\n",
        "    There are two vesions of Capsule.\n",
        "    One is like dense layer (for the fixed-shape input),\n",
        "    and the other is like timedistributed dense (for various length input).\n",
        "\n",
        "    The input shape of Capsule must be (batch_size,\n",
        "                                        input_num_capsule,\n",
        "                                        input_dim_capsule\n",
        "                                       )\n",
        "    and the output shape is (batch_size,\n",
        "                             num_capsule,\n",
        "                             dim_capsule\n",
        "                            )\n",
        "\n",
        "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
        "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 share_weights=True,\n",
        "                 activation='squash',\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(1, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(input_num_capsule, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
        "        but replace b = b + <u,v> with b = <u,v>.\n",
        "\n",
        "        This change can improve the feature representation of Capsule.\n",
        "\n",
        "        However, you can replace\n",
        "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        with\n",
        "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        to realize a standard routing.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule,\n",
        "                                self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
        "            if i < self.routings - 1:\n",
        "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
        "                if K.backend() == 'theano':\n",
        "                    o = K.sum(o, axis=1)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEDF7z9EyLR_"
      },
      "source": [
        "k=0\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "n=50 #set n equal number of authors present in the dataset\n",
        "p=50 #set p equal to number of tweets per user you want in training set\n",
        "z=100 #Z is the total number of tweets per user\n",
        "for i in range(0,n):  \n",
        "  for j in range(k,k+p):\n",
        "    X_train.append(dataset.iloc[j,0])\n",
        "    y_train.append(dataset.iloc[j,1])\n",
        "  k+=z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "7V-7TfWyySun",
        "outputId": "eb8ba81c-41da-456d-ef67-3bd60c21957c"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'India and Benin should broaden trade ties: T...</td>\n",
              "      <td>61771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'Parliament should have 100 sittings a year: ...</td>\n",
              "      <td>61771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b\"PM launches Cairn India's oil fields in Barm...</td>\n",
              "      <td>61771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'Jharkhand governor to hold additional charge...</td>\n",
              "      <td>61771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'British national found dead in Goa village -...</td>\n",
              "      <td>61771813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>b\"@SelenaSly2 ooc: *hugs* I'm Great ! Merry Ch...</td>\n",
              "      <td>63611401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>b'@SlytherinReject thanks *smiles slightly* an...</td>\n",
              "      <td>63611401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>b'@jadeeslytherin It was good !  good !\\n'</td>\n",
              "      <td>63611401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>b\"@aceravenclaw *laughs and starts to calm dow...</td>\n",
              "      <td>63611401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>b'@AshSlytherin15 ooc: lollz , what other type...</td>\n",
              "      <td>63611401</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0         1\n",
              "0     b'India and Benin should broaden trade ties: T...  61771813\n",
              "1     b'Parliament should have 100 sittings a year: ...  61771813\n",
              "2     b\"PM launches Cairn India's oil fields in Barm...  61771813\n",
              "3     b'Jharkhand governor to hold additional charge...  61771813\n",
              "4     b'British national found dead in Goa village -...  61771813\n",
              "...                                                 ...       ...\n",
              "4995  b\"@SelenaSly2 ooc: *hugs* I'm Great ! Merry Ch...  63611401\n",
              "4996  b'@SlytherinReject thanks *smiles slightly* an...  63611401\n",
              "4997         b'@jadeeslytherin It was good !  good !\\n'  63611401\n",
              "4998  b\"@aceravenclaw *laughs and starts to calm dow...  63611401\n",
              "4999  b'@AshSlytherin15 ooc: lollz , what other type...  63611401\n",
              "\n",
              "[5000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEQUBAniyqLC"
      },
      "source": [
        "df = pd.DataFrame({'Text':X_train,'Author':y_train})\n",
        "df.to_csv('50_tweets_per_user.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHo7vGhNLXSM"
      },
      "source": [
        " \n",
        "dataset=pd.read_csv('/content/100_tweets_per_user.csv')  #the location of the dataset that need to be processed\n",
        "\n",
        "k=0\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "n=50 #set n equal number of authors present in the dataset\n",
        "p=50 #set p equal to number of tweets per user you want in training set\n",
        "z=100 #Z is the total number of tweets per user\n",
        "for i in range(0,n):  \n",
        "  for j in range(k,k+p):\n",
        "    X_train.append(dataset.iloc[j,0])\n",
        "    y_train.append(dataset.iloc[j,1])\n",
        "  k+=z\n",
        "\n",
        "a=10 #set a equal to number of tweets per user you want in testing set\n",
        "k=p\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "for i in range(0,n):\n",
        "  for j in range(k,k+a):\n",
        "    X_test.append(dataset.iloc[j,0])\n",
        "    y_test.append(dataset.iloc[j,1])\n",
        "  k+=z\n",
        "\n",
        "X_train=pd.DataFrame(X_train)\n",
        "X_test=pd.DataFrame(X_test)\n",
        "y_train=pd.DataFrame(y_train)\n",
        "y_test=pd.DataFrame(y_test)\n",
        "X_train=X_train.iloc[:,:].values\n",
        "X_test=X_test.iloc[:,:].values\n",
        "y_train=y_train.iloc[:,:].values\n",
        "y_test=y_test.iloc[:,:].values\n",
        "\n",
        "train=np.concatenate((X_train,y_train),axis=1)\n",
        "test=np.concatenate((X_test,y_test),axis=1)\n",
        "\n",
        "np.random.shuffle(train)\n",
        "np.random.shuffle(test)\n",
        "\n",
        "train=pd.DataFrame(train)\n",
        "test=pd.DataFrame(test)\n",
        "\n",
        "X_train=train.iloc[:,0]\n",
        "X_test=test.iloc[:,0]\n",
        "y_train=train.iloc[:,1]\n",
        "y_test=test.iloc[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEa08dZCyQ1n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgI73qDoNIEF"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_Y = encoder.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_train = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_test)\n",
        "encoded_Y = encoder.transform(y_test)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_test = np_utils.to_categorical(encoded_Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiubhXakLpxT"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zjqzHiRKgB"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "vectorizer.transform(X_train)\n",
        "vectorizer.transform(X_test)\n",
        "tfidf_vector_X = vectorizer.transform(X_train).toarray() \n",
        "tfidf_vector_Y = vectorizer.transform(X_test).toarray() \n",
        "#tfidf_vector_X = tfidf_vector_X[:, :, None] \n",
        "#tfidf_vector_Y = tfidf_vector_Y[:, :, None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzuj73ulGIN-",
        "outputId": "53490101-ea69-4586-e40b-18477a2ecaf9"
      },
      "source": [
        "tfidf_vector_X.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_oF-uViNZWk"
      },
      "source": [
        "# Splitting of training set into training set and evalution set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(tfidf_vector_X,y_train, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryixryxHR-YK",
        "outputId": "8287f568-9724-4922-dfc9-334e804affdc"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 7814, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfbDrOzjNql-"
      },
      "source": [
        "# Early Stoppping\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3) \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0EXc4e2OFuX"
      },
      "source": [
        "def model2(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter,cat_output):                                                  # For Character Embedding use this model instead of above model\n",
        "    d = 300                                                             #Embedding Size\n",
        "    inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')\n",
        "    conv = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0], activation='relu',input_shape=(maxlen, vocab_size))(inputs)\n",
        "\n",
        "    #conv=KernelConv2D(input_shape=(maxlen ,vocab_size),filters=128,kernel_size=1,kernel_function=PolynomialKernel(p=2, trainable_c=True))(inputs)\n",
        "    #conv = MaxPooling1D(pool_length=3)(conv)\n",
        "\n",
        "    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1],\n",
        "                           activation='relu')(conv)\n",
        "    #conv1 = MaxPooling1D(pool_length=3)(conv1)\n",
        "\n",
        "    #conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],border_mode='valid', activation='relu')(conv1)\n",
        "    #conv5 = MaxPooling1D(pool_length=3)(conv2)\n",
        "    conv6= Capsule(num_capsule=1 ,dim_capsule=72, routings=1,share_weights=True)(conv1)\n",
        "    conv = Flatten()(conv6)\n",
        "\n",
        "    #Two dense layers with dropout of .5\n",
        "    #z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv))\n",
        "    # z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))\n",
        "\n",
        "    pred = Dense(cat_output, activation='softmax', name='output')(conv)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=pred)\n",
        "\n",
        "    #sgd = SGD(lr=0.001, momentum=0.9)\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSYifJBNOU2h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1ec94f-aa5a-4686-a37c-4f8b6132e44f"
      },
      "source": [
        "nb_filter = 500\n",
        "dense_outputs = 256\n",
        "filter_kernels = [3,4,5]\n",
        "cat_output = 50 #Set cat_output equal to number of authors present in the dataset\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model = model2(filter_kernels, dense_outputs,7814, 1, nb_filter, cat_output)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 7814, 1)]         0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 7812, 500)         2000      \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 7809, 500)         1000500   \n",
            "_________________________________________________________________\n",
            "capsule (Capsule)            (None, 1, 1, 72)          36000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 72)                0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 50)                3650      \n",
            "=================================================================\n",
            "Total params: 1,042,150\n",
            "Trainable params: 1,042,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8i8bzaOjWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8a707e-0cf3-4e9a-c32a-f8e49096790d"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "model.fit(np.array(X_train), np.array(y_train),validation_data=(np.array(X_eval), np.array(y_eval)),batch_size=32,epochs=100, verbose=1,callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 6/47 [==>...........................] - ETA: 15:29 - loss: 3.9157 - accuracy: 0.0156"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC6Y9baQOo4_"
      },
      "source": [
        "# Prediction\n",
        "y_pred = model.predict(np.array(tfidf_vector_Y))\n",
        "\n",
        "y_pred=pd.DataFrame(y_pred)\n",
        "y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)\n",
        "y_pred=y_pred.iloc[:,:].values\n",
        "\n",
        "y_test=pd.DataFrame(y_test)\n",
        "y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)\n",
        "y_test=y_test.iloc[:,:].values\n",
        "\n",
        "result=[]\n",
        "for i in range(0,len(y_test)):\n",
        "  for j in range(0,len(y_test[0])):\n",
        "    if(y_test[i][j]==1):\n",
        "      result.append(j)\n",
        "\n",
        "predicted=[]\n",
        "for i in range(0,len(y_pred)):\n",
        "  for j in range(0,len(y_pred[0])):\n",
        "    if(y_pred[i][j]==1):\n",
        "      predicted.append(j)\n",
        "\n",
        "print(result)\n",
        "print(predicted)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(result,predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "print('Confusion Matrix :')\n",
        "print(cm) \n",
        "print('Accuracy Score :',accuracy_score(result, predicted)) \n",
        "print('Report : ')\n",
        "print(classification_report(result, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}