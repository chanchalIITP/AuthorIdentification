{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fastext+Bigram_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh433o_NJXpQ"
      },
      "source": [
        "import os\n",
        "import math\n",
        "from termcolor import colored\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import pandas as pd\n",
        "#from data_helpers import load_data\n",
        "from keras import callbacks\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.layers import Embedding\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Dropout, Flatten\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "\n",
        "import regex\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D, AveragePooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjh1PgjuJ6dM",
        "outputId": "691ef0ac-6dbd-4d2e-a58d-4e2278ebcbaa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT3J1zIsp7t2"
      },
      "source": [
        "#Necessary imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.keras import activations, regularizers, initializers, constraints, engine\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.keras.layers import Layer, deserialize, Conv1D\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.keras.models import Sequential,Model\n",
        "from tensorflow.python.keras.layers import Flatten, Dense,AveragePooling1D,GRU,Convolution1D, MaxPooling1D, AveragePooling1D,Embedding,Input, Dense, Dropout, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MK82Ge3Jird",
        "outputId": "19cd55c8-941b-4add-871b-ba3d3d7b599b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "import os, re, csv, math, codecs\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "import nltk\n",
        "!nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "MAX_NB_WORDS = 100000\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'stopwords''\n",
            "/bin/bash: -c: line 0: `nltk.download('stopwords')'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2JqCcrnJ2PQ",
        "outputId": "7a3a0085-b961-4e30-f7a1-031d296701bc"
      },
      "source": [
        "!unzip /content/drive/MyDrive/wiki.simple.vec.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/wiki.simple.vec.zip\n",
            "replace wiki.simple.vec? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wiki.simple.vec         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt7PkucIKDzF",
        "outputId": "857a85ca-51ea-4af4-cc45-f9f60b3e872d"
      },
      "source": [
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open('wiki.simple.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1507it [00:00, 7647.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading word embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "111052it [00:14, 7655.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 111052 word vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHn8k0auKJdf"
      },
      "source": [
        "dataset=pd.read_csv('Abstract_Author.csv')\n",
        "\n",
        "\n",
        "k=0\n",
        "X_train=[]\n",
        "y_train=[]\n",
        "n=155 #set n equal number of authors present in the dataset\n",
        "p=40 #set p equal to number of tweets per user you want in training set\n",
        "z=50 #Z is the total number of tweets per user\n",
        "for i in range(0,n):  \n",
        "  for j in range(k,k+p):\n",
        "    X_train.append(dataset.iloc[j,0])\n",
        "    y_train.append(dataset.iloc[j,1])\n",
        "  k+=z\n",
        "\n",
        "a=10 #set a equal to number of tweets per user you want in testing set\n",
        "k=p\n",
        "X_test=[]\n",
        "y_test=[]\n",
        "for i in range(0,n):\n",
        "  for j in range(k,k+a):\n",
        "    X_test.append(dataset.iloc[j,0])\n",
        "    y_test.append(dataset.iloc[j,1])\n",
        "  k+=z\n",
        "\n",
        "X_train=pd.DataFrame(X_train)\n",
        "X_test=pd.DataFrame(X_test)\n",
        "y_train=pd.DataFrame(y_train)\n",
        "y_test=pd.DataFrame(y_test)\n",
        "X_train=X_train.iloc[:,:].values\n",
        "X_test=X_test.iloc[:,:].values\n",
        "y_train=y_train.iloc[:,:].values\n",
        "y_test=y_test.iloc[:,:].values\n",
        "\n",
        "train=np.concatenate((X_train,y_train),axis=1)\n",
        "test=np.concatenate((X_test,y_test),axis=1)\n",
        "\n",
        "np.random.shuffle(train)\n",
        "np.random.shuffle(test)\n",
        "\n",
        "train=pd.DataFrame(train)\n",
        "test=pd.DataFrame(test)\n",
        "\n",
        "X_train1=train.iloc[:,0]\n",
        "X_test=test.iloc[:,0]\n",
        "y_train=train.iloc[:,1]\n",
        "y_test=test.iloc[:,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5am8weWRqpl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frSltOmPKRY_"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_train)\n",
        "encoded_Y = encoder.transform(y_train)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_train = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(y_test)\n",
        "encoded_Y = encoder.transform(y_test)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "y_test = np_utils.to_categorical(encoded_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ldWxqPHLA3B"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X_train1,y_train, test_size = 0.05, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyyInhZLLLVg",
        "outputId": "bedab983-d697-442c-f0a5-b24362131ca3"
      },
      "source": [
        "print(\"tokenizing input data...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(X_train1)  #leaky\n",
        "word_seq_train = tokenizer.texts_to_sequences(X_train)\n",
        "word_seq_test = tokenizer.texts_to_sequences(X_test)\n",
        "word_seq_eval=tokenizer.texts_to_sequences(X_eval)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"dictionary size: \", len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizing input data...\n",
            "dictionary size:  20554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvIR2aNNL-vC"
      },
      "source": [
        "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=100)\n",
        "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=100)\n",
        "word_seq_eval = sequence.pad_sequences(word_seq_eval, maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTRDyPJ4MMA0"
      },
      "source": [
        "embed_dim = 300 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAeB83ZOMd20",
        "outputId": "31ac8add-7161-4235-9f59-344a0fdcd0d7"
      },
      "source": [
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 8547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CfeieNsMhbM"
      },
      "source": [
        "def create_vocab_set2():\n",
        "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
        "                list(string.punctuation) + ['\\n'] + [' '] )\n",
        "    \n",
        "    print('alphabet', alphabet)\n",
        "    #x = input(print('enter'))\n",
        "    alphabets = []\n",
        "    for i in range(0, len(alphabet)):\n",
        "        for j in range(0, len(alphabet)):\n",
        "              alphabets.append(alphabet[i]+alphabet[j])\n",
        "    #print(alphabets)\n",
        "    #x = input(print('enter'))\n",
        "\n",
        "\n",
        "    vocab_size = len(alphabets)\n",
        "    check = set(alphabets)\n",
        "\n",
        "    vocab = {}\n",
        "    reverse_vocab = {}\n",
        "    for ix, t in enumerate(alphabets):\n",
        "        vocab[t] = ix\n",
        "        reverse_vocab[ix] = t\n",
        "\n",
        "    return vocab, reverse_vocab, vocab_size, check\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfwpU4xMLP8v"
      },
      "source": [
        "def create_vocab_set():\n",
        "    alphabet = (list(string.ascii_lowercase) + list(string.digits) +\n",
        "                list(string.punctuation) + ['\\n'] + [' '] )\n",
        "    vocab_size = len(alphabet)\n",
        "    check = set(alphabet)\n",
        "\n",
        "    vocab = {}\n",
        "    reverse_vocab = {}\n",
        "    for ix, t in enumerate(alphabet):\n",
        "        vocab[t] = ix\n",
        "        reverse_vocab[ix] = t\n",
        "\n",
        "    return vocab, reverse_vocab, vocab_size, check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3hZvAgaNGC9"
      },
      "source": [
        "maxlen = 100\n",
        "vocab, reverse_vocab, vocab_size, check =create_vocab_set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf34N0SoNJ6b"
      },
      "source": [
        "def encode_data2(x, maxlen, vocab, vocab_size, check):              # For character embedding use this function instead of encode_data\n",
        "\n",
        "    input_data = np.zeros((len(x), maxlen))\n",
        "    for dix, sent in enumerate(x):\n",
        "        counter = 0\n",
        "        chars = list(sent.lower().replace(\" \", \"\"))\n",
        "        chars2 = []\n",
        "        for i in range(len(chars)-1):\n",
        "            chars2.append(chars[i] + chars[i+1])\n",
        "        for i,c in enumerate(chars2):\n",
        "            if counter >= maxlen:\n",
        "                pass\n",
        "            else:\n",
        "                if c in check:\n",
        "                    counter += 1\n",
        "                    ix = vocab[c]\n",
        "                    input_data[dix,counter-1] = ix\n",
        "\n",
        "    return input_data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38HslePQKx_U"
      },
      "source": [
        "def encode_data(x, maxlen, vocab, vocab_size, check):\n",
        "\n",
        "    input_data = np.zeros((len(x), maxlen, vocab_size))\n",
        "    for dix, sent in enumerate(x):\n",
        "        counter = 0\n",
        "        sent_array = np.zeros((maxlen, vocab_size))\n",
        "        chars = list(sent.lower())\n",
        "        for c in chars:\n",
        "            if counter >= maxlen:\n",
        "                pass\n",
        "            else:\n",
        "                char_array = np.zeros(vocab_size, dtype=np.int)\n",
        "                if c in check:\n",
        "                    ix = vocab[c]\n",
        "                    char_array[ix] = 1\n",
        "                sent_array[counter, :] = char_array\n",
        "                counter += 1\n",
        "        input_data[dix, :, :] = sent_array\n",
        "\n",
        "    return input_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQnU5J2hNWb3"
      },
      "source": [
        "char_seq_train = encode_data2(X_train, maxlen, vocab, vocab_size, check)\n",
        "char_seq_test = encode_data2(X_test, maxlen, vocab, vocab_size, check)\n",
        "char_seq_eval = encode_data2(X_eval, maxlen, vocab, vocab_size, check)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b0fEyqwsBZK",
        "outputId": "ff2f4128-b28c-4be4-fabe-c8bf2398b5b8"
      },
      "source": [
        "char_seq_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5890, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEdfUp0WNqvM"
      },
      "source": [
        "def model(filter_kernels, dense_outputs, maxlen, vocab_size, nb_filter,cat_output,nb_words,embed_dim):                                                  # For Character Embedding use this model instead of above model\n",
        "    d = 300                                                             #Embedding Size\n",
        "    Embedding_layer  = Embedding(vocab_size+1, d, input_length=maxlen)\n",
        "    input1 = Input(shape=(maxlen,), name='input', dtype='float32')\n",
        "    embed = Embedding_layer(input1)\n",
        "    conv = Convolution1D(filters=nb_filter , kernel_size=filter_kernels[0], activation='relu',input_shape=(maxlen, d))(embed)\n",
        "    conv = MaxPooling1D(pool_size=3)(conv)\n",
        "    conv1 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1],activation='relu')(conv)\n",
        "    conv1 = MaxPooling1D(pool_size=3)(conv1)\n",
        "    conv2 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2],activation='relu')(conv1)\n",
        "    conv2 = MaxPooling1D(pool_size=3)(conv2)\n",
        "    character = Flatten()(conv2)\n",
        "\n",
        "    input2 = Input(shape=(100,))\n",
        "    embed2=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=140, trainable=False)(input2)\n",
        "    conv3 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[0], activation='relu')(embed2)\n",
        "    conv3 = MaxPooling1D(pool_size=3)(conv3)\n",
        "    conv4 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[1],activation='relu')(conv3)\n",
        "    conv4 = MaxPooling1D(pool_size=3)(conv4)\n",
        "    conv5 = Convolution1D(filters=nb_filter, kernel_size=filter_kernels[2],activation='relu')(conv4)\n",
        "    conv5 = MaxPooling1D(pool_size=3)(conv5)\n",
        "    word = Flatten()(conv5)\n",
        "    \n",
        "    \n",
        "    merged = concatenate([character, word])\n",
        "\n",
        "\n",
        "    pred = Dense(155, activation='softmax', name='output')(merged)\n",
        "\n",
        "    model = Model(inputs=[input1,input2] , outputs=pred)\n",
        "\n",
        "    #sgd = SGD(lr=0.001, momentum=0.9)\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRhl_SSyrHo1"
      },
      "source": [
        "from tensorflow.keras.layers import concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkv6Lja1RtzS",
        "outputId": "770c52b1-c342-4292-957b-9108947af755"
      },
      "source": [
        "nb_filter = 450\n",
        "dense_outputs = 256\n",
        "filter_kernels = [3,4,5]\n",
        "cat_output = 50\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model = model(filter_kernels, dense_outputs,maxlen, vocab_size, nb_filter, cat_output,nb_words,embed_dim)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 100, 300)     21300       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, 100, 300)     6166500     input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 98, 450)      405450      embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 98, 450)      405450      embedding_13[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_34 (MaxPooling1D) (None, 32, 450)      0           conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_37 (MaxPooling1D) (None, 32, 450)      0           conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 29, 450)      810450      max_pooling1d_34[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 29, 450)      810450      max_pooling1d_37[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_35 (MaxPooling1D) (None, 9, 450)       0           conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_38 (MaxPooling1D) (None, 9, 450)       0           conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 5, 450)       1012950     max_pooling1d_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_41 (Conv1D)              (None, 5, 450)       1012950     max_pooling1d_38[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_36 (MaxPooling1D) (None, 1, 450)       0           conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_39 (MaxPooling1D) (None, 1, 450)       0           conv1d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 450)          0           max_pooling1d_36[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_13 (Flatten)            (None, 450)          0           max_pooling1d_39[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 900)          0           flatten_12[0][0]                 \n",
            "                                                                 flatten_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 155)          139655      concatenate_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 10,785,155\n",
            "Trainable params: 4,618,655\n",
            "Non-trainable params: 6,166,500\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB_1ZFtrST0j"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqPzK7H2Tkqv"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3) \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYUtCPE8S3Hl"
      },
      "source": [
        "model.fit([np.array(char_seq_train),np.array(word_seq_train)] , np.array(y_train),validation_data=([np.array(char_seq_eval),np.array(word_seq_eval)] , np.array(y_eval)),batch_size=16,epochs=5, verbose=1,callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOvjY9ElTiVU"
      },
      "source": [
        "y_pred = model.predict([np.array(char_seq_test),np.array(word_seq_test)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRyqZVycF8v",
        "outputId": "fd09e5b4-1f0a-4942-f33c-5363758872dd"
      },
      "source": [
        "y_pred=pd.DataFrame(y_pred)\n",
        "y_pred=y_pred.eq(y_pred.where(y_pred != 0).max(1), axis=0).astype(int)\n",
        "y_pred=y_pred.iloc[:,:].values\n",
        "\n",
        "y_test=pd.DataFrame(y_test)\n",
        "y_test=y_test.eq(y_test.where(y_test != 0).max(1), axis=0).astype(int)\n",
        "y_test=y_test.iloc[:,:].values\n",
        "\n",
        "result=[]\n",
        "for i in range(0,len(y_test)):\n",
        "  for j in range(0,len(y_test[0])):\n",
        "    if(y_test[i][j]==1):\n",
        "      result.append(j)\n",
        "\n",
        "predicted=[]\n",
        "for i in range(0,len(y_pred)):\n",
        "  for j in range(0,len(y_pred[0])):\n",
        "    if(y_pred[i][j]==1):\n",
        "      predicted.append(j)\n",
        "\n",
        "print(result)\n",
        "print(predicted)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(result,predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report \n",
        "print('Confusion Matrix :')\n",
        "print(cm) \n",
        "print('Accuracy Score :',accuracy_score(result, predicted)) \n",
        "print('Report : ')\n",
        "print(classification_report(result, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 27, 10, 3, 23, 24, 28, 44, 11, 33, 49, 40, 30, 13, 28, 1, 22, 20, 2, 20, 13, 20, 49, 6, 38, 3, 16, 23, 14, 16, 27, 31, 6, 48, 39, 16, 13, 9, 33, 26, 1, 38, 12, 49, 30, 21, 49, 18, 39, 19, 31, 35, 46, 29, 0, 34, 28, 0, 42, 33, 33, 42, 43, 5, 31, 44, 1, 43, 47, 12, 6, 14, 20, 3, 41, 25, 45, 34, 46, 8, 6, 35, 24, 10, 13, 29, 49, 29, 7, 19, 2, 15, 15, 24, 14, 25, 4, 13, 4, 26, 37, 7, 5, 38, 27, 47, 7, 30, 40, 5, 45, 16, 41, 17, 21, 32, 22, 36, 35, 25, 9, 19, 22, 41, 45, 4, 26, 0, 43, 2, 11, 11, 1, 8, 37, 39, 28, 12, 41, 24, 22, 47, 5, 48, 10, 3, 14, 42, 10, 47, 17, 47, 30, 12, 4, 36, 22, 45, 29, 46, 27, 14, 32, 30, 36, 43, 11, 24, 40, 23, 38, 2, 18, 36, 32, 20, 25, 43, 3, 25, 7, 32, 15, 12, 17, 26, 17, 48, 9, 4, 8, 21, 37, 34, 1, 0, 15, 18, 31, 34, 44, 5, 8, 26, 23, 41, 35, 34, 9, 46, 35, 45, 17, 44, 18, 6, 16, 2, 29, 38, 23, 31, 10, 40, 7, 40, 39, 27, 39, 15, 19, 18, 48, 44, 37, 42, 48, 11, 21, 37, 8, 42, 32, 33, 28, 0, 21, 9, 46, 36]\n",
            "[19, 27, 25, 3, 23, 40, 28, 44, 2, 33, 49, 40, 30, 13, 28, 1, 20, 20, 5, 20, 13, 20, 49, 6, 13, 15, 16, 23, 14, 42, 27, 31, 6, 48, 39, 16, 13, 22, 33, 43, 1, 23, 8, 49, 30, 24, 49, 22, 39, 11, 31, 35, 46, 29, 0, 34, 28, 38, 7, 33, 33, 42, 43, 5, 31, 44, 1, 43, 47, 19, 6, 14, 20, 43, 41, 11, 45, 34, 46, 36, 6, 35, 32, 21, 2, 29, 49, 29, 42, 21, 5, 10, 21, 32, 14, 19, 4, 13, 4, 26, 37, 3, 5, 18, 27, 47, 7, 30, 40, 14, 45, 15, 41, 17, 12, 5, 22, 36, 35, 21, 9, 19, 21, 17, 26, 4, 5, 0, 27, 26, 5, 11, 1, 32, 37, 39, 28, 15, 41, 25, 22, 47, 25, 48, 10, 3, 14, 42, 24, 47, 17, 47, 30, 16, 4, 36, 26, 45, 29, 46, 27, 14, 12, 30, 36, 20, 38, 24, 40, 23, 5, 18, 22, 36, 39, 20, 12, 43, 3, 25, 6, 32, 12, 15, 17, 26, 17, 48, 29, 4, 19, 19, 37, 34, 1, 36, 15, 24, 31, 34, 39, 5, 9, 16, 33, 41, 28, 34, 27, 46, 35, 45, 17, 44, 9, 6, 19, 18, 29, 43, 23, 31, 19, 40, 7, 40, 39, 21, 39, 15, 32, 26, 48, 44, 37, 42, 48, 36, 42, 37, 8, 3, 32, 33, 28, 10, 16, 9, 46, 36]\n",
            "Confusion Matrix :\n",
            "[[2 0 0 ... 0 0 0]\n",
            " [0 5 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 5 0 0]\n",
            " [0 0 0 ... 0 5 0]\n",
            " [0 0 0 ... 0 0 5]]\n",
            "Accuracy Score : 0.652\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.40      0.57         5\n",
            "           1       1.00      1.00      1.00         5\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.60      0.60      0.60         5\n",
            "           4       1.00      1.00      1.00         5\n",
            "           5       0.33      0.60      0.43         5\n",
            "           6       0.83      1.00      0.91         5\n",
            "           7       0.67      0.40      0.50         5\n",
            "           8       0.50      0.20      0.29         5\n",
            "           9       0.50      0.40      0.44         5\n",
            "          10       0.33      0.20      0.25         5\n",
            "          11       0.33      0.20      0.25         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.80      0.80      0.80         5\n",
            "          14       0.83      1.00      0.91         5\n",
            "          15       0.33      0.40      0.36         5\n",
            "          16       0.40      0.40      0.40         5\n",
            "          17       0.83      1.00      0.91         5\n",
            "          18       0.00      0.00      0.00         5\n",
            "          19       0.25      0.40      0.31         5\n",
            "          20       0.71      1.00      0.83         5\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.40      0.40      0.40         5\n",
            "          23       0.80      0.80      0.80         5\n",
            "          24       0.25      0.20      0.22         5\n",
            "          25       0.25      0.20      0.22         5\n",
            "          26       0.33      0.40      0.36         5\n",
            "          27       0.67      0.80      0.73         5\n",
            "          28       0.83      1.00      0.91         5\n",
            "          29       0.83      1.00      0.91         5\n",
            "          30       1.00      1.00      1.00         5\n",
            "          31       1.00      1.00      1.00         5\n",
            "          32       0.33      0.40      0.36         5\n",
            "          33       0.83      1.00      0.91         5\n",
            "          34       1.00      1.00      1.00         5\n",
            "          35       1.00      0.80      0.89         5\n",
            "          36       0.62      1.00      0.77         5\n",
            "          37       1.00      1.00      1.00         5\n",
            "          38       0.00      0.00      0.00         5\n",
            "          39       0.71      1.00      0.83         5\n",
            "          40       0.83      1.00      0.91         5\n",
            "          41       1.00      0.80      0.89         5\n",
            "          42       0.50      0.60      0.55         5\n",
            "          43       0.50      0.60      0.55         5\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       1.00      0.80      0.89         5\n",
            "          46       1.00      1.00      1.00         5\n",
            "          47       1.00      1.00      1.00         5\n",
            "          48       1.00      1.00      1.00         5\n",
            "          49       1.00      1.00      1.00         5\n",
            "\n",
            "    accuracy                           0.65       250\n",
            "   macro avg       0.64      0.65      0.63       250\n",
            "weighted avg       0.64      0.65      0.63       250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}